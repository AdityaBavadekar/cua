---
title: HUD Evals
description: Use ComputerAgent with HUD for benchmarking and evaluation
---

The HUD integration allows you to use ComputerAgent with the [HUD benchmarking framework](https://www.hud.so/), providing the same interface as existing HUD agents while leveraging ComputerAgent's capabilities.

## Installation

```bash
pip install "cua-agent[hud]"
## or install hud-python directly
# pip install hud-python==0.4.12
```

## Usage

```python
# Quick single-task smoke test
from agent.integrations.hud import run_single_task

await run_single_task(
    dataset="hud-evals/OSWorld-Verified-XLang",   # or another HUD dataset
    model="openai/computer-use-preview+openai/gpt-5-nano",  # any supported model string
    task_id=155,  # e.g., reopen last closed tab
)

# Run a small split of OSWorld-Verified in parallel
from agent.integrations.hud import run_full_dataset

results = await run_full_dataset(
    dataset="hud-evals/OSWorld-Verified-XLang",   # can also pass a Dataset or list[dict]
    model="openai/computer-use-preview",
    split="train[:3]",           # try a few tasks to start
    max_concurrent=20,            # tune to your infra
    max_steps=50                  # safety cap per task
)

# Environment variables required:
# - HUD_API_KEY (HUD access)
# - OPENAI_API_KEY or ANTHROPIC_API_KEY depending on your chosen model(s)
```

**Available Benchmarks:**
1. [OSWorld-Verified](/agent-sdk/benchmarks/osworld-verified) - Benchmark on OSWorld tasks

See the [HUD docs](https://docs.hud.so/environment-creation) for more eval environments.