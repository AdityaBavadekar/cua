---
title: Set-of-Mark
macos: true
pypi: cua-computer
---

import { buttonVariants } from 'fumadocs-ui/components/ui/button';
import { cn } from 'fumadocs-ui/utils/cn';
import { ChevronRight } from 'lucide-react';

## Overview

**Set-of-Mark (Som)** is a high-performance visual grounding library for detecting and analyzing UI elements in screenshots. Built for the Computer-Use Agent (CUA) framework, it combines state-of-the-art computer vision models to identify icons, buttons, and text in user interfaces.

<Callout type="info">
  Som is optimized for **Apple Silicon** with Metal Performance Shaders (MPS)
  acceleration, achieving sub-second detection times while maintaining high
  accuracy.
</Callout>

### Key Features

- **Hardware Acceleration** - Automatic detection of MPS, CUDA, or CPU
- **Multi-Model Architecture** - YOLO for icons + EasyOCR for text
- **Optimized Performance** - Sub-second detection on Apple Silicon
- **Flexible Configuration** - Tunable thresholds for different use cases
- **Rich Output Format** - Structured data with confidence scores
- **Visual Debugging** - Annotated screenshots with numbered elements

## Installation

<Callout type="warning">
  Som requires Python 3.11 or higher. For best performance, use macOS with Apple
  Silicon.
</Callout>

### Install from PyPI

```bash
pip install cua-som
```

### Install from Source

```bash
# Clone the repository
git clone https://github.com/cua/som.git
cd som

# Using PDM (recommended)
pdm install

# Or using pip
pip install -e .
```

### System Requirements

| Platform      | Hardware                 | Detection Time |
| ------------- | ------------------------ | -------------- |
| macOS         | Apple Silicon (M1/M2/M3) | ~0.4s          |
| Linux/Windows | NVIDIA GPU               | ~0.6s          |
| Any           | CPU only                 | ~1.3s          |

## Getting Started

### Basic Usage

Here's a simple example to detect UI elements in a screenshot:

```python
from som import OmniParser
from PIL import Image

# Initialize the parser
parser = OmniParser()

# Load and process an image
image = Image.open("screenshot.png")
result = parser.parse(image)

# Print detected elements
for elem in result.elements:
    print(f"{elem.type}: {elem.content or 'icon'} at {elem.bbox.coordinates}")
```

### Advanced Configuration

Customize detection parameters for your specific use case:

```python
result = parser.parse(
    image,
    box_threshold=0.3,    # Confidence threshold (0.0-1.0)
    iou_threshold=0.1,    # Overlap threshold (0.0-1.0)
    use_ocr=True,         # Enable text detection
    ocr_engine="easyocr" # OCR engine choice
)
```

### Working with Results

```python
# Filter by element type
icons = [e for e in result.elements if e.type == "icon"]
texts = [e for e in result.elements if e.type == "text"]

# Get high-confidence detections
high_conf = [e for e in result.elements if e.confidence > 0.8]

# Access bounding boxes
for elem in result.elements:
    x, y, w, h = elem.bbox.coordinates
    print(f"Element at ({x}, {y}) with size {w}x{h}")
```

<Callout>
  ðŸ’¡ **Tip**: Check out our [interactive examples](#examples) below to see Som
  in action with real UI screenshots.
</Callout>

## Configuration Guide

### Detection Parameters

<Cards>
  <Card
    title="Box Threshold"
    description="Controls detection confidence (default: 0.3)">
    - **Higher values (0.4-0.5)**: More precise, fewer false positives - **Lower
    values (0.1-0.2)**: More detections, may include noise - **Recommended**:
    0.3 for balanced performance
  </Card>

  <Card
    title="IOU Threshold"
    description="Controls overlap handling (default: 0.1)">
    - **Lower values (0.05-0.1)**: Aggressive merging of overlaps - **Higher
    values (0.3-0.5)**: Keeps more overlapping boxes - **Recommended**: 0.1 for
    dense UIs
  </Card>
</Cards>

### OCR Settings

| Setting        | Default   | Description              |
| -------------- | --------- | ------------------------ |
| Engine         | `easyocr` | OCR engine to use        |
| Languages      | `['en']`  | Supported languages      |
| GPU            | `auto`    | Enable GPU acceleration  |
| Timeout        | `5s`      | Maximum processing time  |
| Min Confidence | `0.5`     | Text detection threshold |

## Performance Benchmarks

### Hardware Acceleration

<Mermaid
  chart="
graph LR
    A[Input Image] --> B{Hardware Detection}
    B -->|Apple Silicon| C[MPS Backend]
    B -->|NVIDIA GPU| D[CUDA Backend]
    B -->|No GPU| E[CPU Backend]
    C --> F[Multi-scale\n~0.4s]
    D --> G[Multi-scale\n~0.6s]
    E --> H[Single-scale\n~1.3s]
"
/>

### Optimization Details

<Cards>
  <Card title="MPS (Apple Silicon)" description="Best performance on macOS">
    - Multi-scale detection (640px, 1280px, 1920px) - Test-time augmentation -
    Half-precision (FP16) - ~0.4s average time
  </Card>

{' '}

<Card title="CUDA (NVIDIA)" description="High performance on Linux/Windows">
  - Multi-scale detection - Mixed precision - ~0.6s average time
</Card>

  <Card title="CPU Fallback" description="Universal compatibility">
    - Single-scale detection (1280px) - Full precision (FP32) - ~1.3s average
    time
  </Card>
</Cards>

## Examples

### Example 1: Screenshot Analysis

Analyze a full application screenshot with visualization:

```python
from som import OmniParser
from PIL import Image
import json

# Initialize parser
parser = OmniParser()

# Load screenshot
image = Image.open("app_screenshot.png")

# Parse with visualization
result = parser.parse(image, use_ocr=True)

# Save annotated image
result.visualized_image.save("annotated_screenshot.png")

# Export results as JSON
with open("ui_elements.json", "w") as f:
    json.dump(result.to_dict(), f, indent=2)
```

### Example 2: UI Automation Helper

Find and interact with specific UI elements:

```python
def find_button(parser, image, button_text):
    """Find a button by its text content"""
    result = parser.parse(image, use_ocr=True)

    for elem in result.elements:
        if elem.type == "text" and button_text.lower() in elem.content.lower():
            return elem.bbox.coordinates

    return None

# Usage
button_coords = find_button(parser, screenshot, "Submit")
if button_coords:
    x, y, w, h = button_coords
    click_x = x + w // 2
    click_y = y + h // 2
    print(f"Click at ({click_x}, {click_y})")
```

### Example 3: Batch Processing

Process multiple screenshots with performance tracking:

```python
import time
from pathlib import Path

def batch_process(image_dir, output_dir):
    parser = OmniParser()
    image_files = Path(image_dir).glob("*.png")

    results = []
    for img_path in image_files:
        start = time.time()

        image = Image.open(img_path)
        result = parser.parse(image)

        # Save annotated version
        out_path = Path(output_dir) / f"{img_path.stem}_annotated.png"
        result.visualized_image.save(out_path)

        elapsed = time.time() - start
        results.append({
            "file": img_path.name,
            "elements_found": len(result.elements),
            "processing_time": elapsed
        })

        print(f"Processed {img_path.name}: {len(result.elements)} elements in {elapsed:.2f}s")

    return results
```

---

<Callout type="info">
  **Need the full API documentation?** Explore detailed class references, method
  signatures, and advanced configuration options.
  <a
    href="/api/som"
    className={cn(
      buttonVariants({
        color: 'secondary',
      }),
      'no-underline h-10'
    )}>
    View API Reference
    <ChevronRight size={18} />
  </a>
</Callout>
